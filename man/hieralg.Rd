% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hieralg.R
\name{hieralg}
\alias{hieralg}
\title{Segment data into changepoints assuming hierarchical structure}
\usage{
hieralg(data, likelihood = multivariate, penalty = function(x) 0,
  max_segments = ncol(data), allow_parallel = TRUE)
}
\arguments{
\item{data}{matrix for which to find the changepooints}

\item{likelihood}{a function receives the segment matrix as argument
and returns a likelihood estimation. This function is used to calculate the
changepoints that maximize the total likelihood. Depending on the algorithm
being used, this function is likely to be executed many times, in which
case it's also likely to be the bottleneck of the function execution, so
it's advised that this function should have a performant, native
implementation. Defaults to a performant \code{multivariate} estimation.}

\item{penalty}{a function that receives the segment as parameter and returns
the penalty to the segment, directly subtracted from the likelihood
estimated by the other function. The idea if for this to be used as a way
of avoiding undesirable results, e.g. to avoid large segments by provind a
penalty function that penalizes big segments, of the other way arround, by
providing a penalty function that penalizes small segments.}

\item{max_segments}{an integer that defines the maximum amount of segments to
split the data into.}

\item{allow_parallel}{allows parallel execution to take place using the
registered cluster. Assumes a cluster is registered with the \code{foreach}
package. Defaults to TRUE.}
}
\description{
By assuming changepoints follow an hierarchical architecture, this architecture
manages to run faster by not searching all possible branches
}
\details{
Fast algorithm that segments data intoo changepoints, and it does so by
simplifying by reducing the search possibilities by assuming data split in an
hierarchical structure, i.e. a segment found in a first trial is assumed to
contain only segments independent of the rest of the data. This algorithm
usually runs very fast, but is known to yield less accurate results, possibly
not finding the exact changepoints that would maximize likelihood.
}
