---
title: "Segmenting data with Segmentr"
author: "Thales Mello"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Segmenting data with Segmentr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Segmentr is a package that implements a handful of algorithms to segment a given dataset, by finding
the changepoints that maximize the collective likelihood of the segments according to an arbitrary
likelihood function. So, the user of this package has to find an adequate likelihood for
the segmentation problem to be solved, possibly having to penalize it in order to avoid either an
overparameterized or underparameterized model, i.e. one with too many or too few change points,
respectively. Also, it's important to consider the computation time of each algorithm and its
tradeoffs. This *vignette* walks rough the main concepts regarding its usage, using historical
temperature data from Berlin as an example. For this demo, the following packages will be used:

```{r message=FALSE}
require(segmentr)
require(tidyr)
require(tibble)
require(dplyr)
require(lubridate)
require(magrittr)
require(purrr)
```


## Understanding the data

The `berlin` dataset, provided in this package, contains daily temperature measurements
from `r nrow(berlin)` weather stations in Berlin for every day in the years 2010 and 2011,
i.e., a total of `r ncol(berlin)` days. Therefore, every element in the `berlin` has a
temperature measumement in ÂºC, such that each of the `r ncol(berlin)` columns corresponds
to a date, and each of the `r nrow(berlin)` rows corresponds to the weather station the
measurement was collected at. In the table below, it's possible to see the first three
columns, as well as the last three columns, together with the respective stations.

```{r}
data(berlin)
as_tibble(berlin, rownames="station") %>%
  mutate(`..`="..") %>%
  select(station, `2010-01-01`:`2010-01-03`, `..`, `2011-12-29`:`2011-12-31`)
```

In order to grasp the behavior of the weather data, plot the daily average temperature of all the weather stations,
i.e., the mean value of each column in the `berlin` dataset, with its respective date in order
to observe how the average temperature of berlin behaves over time.

```{r}
berlin %>%
  colMeans() %>%
  enframe("time", "temperature") %>%
  mutate_at(vars(time), ymd) %>%
  with(plot(time, temperature))
```

In the graph, the daily temperatures points alternate in upwards and downwards trends,
which suggests it's posisble to fit linear regresions for each of the upwards or downwards
trend segments. So, a "linear regression likelihood" function is proposed, which should
return a higher value the better a linear regression fits in a given segment. A human
observer might be inclined to segment the weather data like the following.

```{r}
plot_results <- function(results, data) {
  dates <- colnames(data) %>% ymd()
  
  data %>%
    colMeans() %>%
    enframe("time", "temperature") %>%
    mutate_at(vars(time), ymd) %>%
    with({
      plot(time, temperature, type="l")
      abline(v=dates[results$changepoints], col="red", lty=2)
    })
}

plot_results(list(changepoints=c(200, 350, 570)), berlin)
```



## Building the likelihood

An adequate likelihood function should be able to rank a given set of possible segments and pick
the best one given an arbitrary criteria. Since the goal is to select segments with a good
linear regression fit, the standard log-likelihood function used by linear regression methods,
the negative of the squared sum of residuals, is a good candidate. However, given the sum of
residuals tend to increase with the amount of points in a segment, the negative mean of the
squared residuals is a better measurement of the goodness of fit of a given segment. So, in
the equation, $L_lm$ is the linear regression likelihood function, $X$ is the set of points
that belong to the segment, $x_i$ and $y_i$ are the points that belong to $X$ for each index $i$,
and $f$ is the best linear regression that fitted the $X$ segment.

$$
L_{lm}(X)=-\sum_{i=1}^n\frac{1}{n}(y_i - f(x_i))^2
$$

A [segment()] likelihood argument requires a function which accepts a candidate
segment matrix, i.e. a subset of the columns in the dataset, and returns the estimated
likelihood for the candidate. Therefore, the `lm_likelihood` function implementation is provided
below, one which obeys the contract by taking a matrix as argument and returning the negative mean
of the squared residuals of a linear regression over the candidate segment. The likelihoods
for a small, a medium, and a large segment are also provided, in order to compare the behavior
of the likelihood function for different sizes of segments.

```{r}
lm_likelihood <- function (data) {
  fit <- t(data) %>%
    as_tibble() %>%
    rowid_to_column() %>%
    gather(station, temperature, -rowid) %>%
    with(lm(temperature ~ rowid))
    
  -mean(fit$residuals ^ 2)
}

c(lm_likelihood(berlin[, 2:3]), lm_likelihood(berlin[, 1:150]), lm_likelihood(berlin))
```

With the likelihood function defined, it can now be applied to [segment()] in order to get the segmentsfor the `berlin` dataset.
Due to the amount of points in the dataset, the time complexity of the `exact` algorithm is prohibitive, so the `hierarchical`
algorithm is used, even though the data doesn't necessarily have a hierarchical structure.

```{r}
results <- segment(
  berlin,
  likelihood = lm_likelihood,
  algorithm = "hierarchical"
)

results
```

```{r}

plot_results(results, berlin)
```

From the segments computed using the bare `lm_likelihood` function, it's possible see many very short segments,
and a very large last segment. This is a result of the algorithm used, as well as the fact the `lm_likelihood`
function tends to favor very short segments, as they usually have smaller residual error. So, to not get
segments too short or too long, it's necessary to penalize the likelihood function for either extremely
short or extremely long lengths.

## Penalizing the likelihood function

To penalize a likelihood function in the Segmentr context is to decrease the return value of the likelihood
function whenever unwanted segments are provided as an input. Typically, this involves
penalizing the likelihood function whenever a very short or a very long segment is provided.

One method of penalization is to subtract the output of the likelihood function with a penalty function which
is a function of the length of the segment. A penalty function is proposed.

$$
p(l) = C_1e^{s_1(l - \frac{L}{2})} + C_2e^{s_2(-l + \frac{L}{2})}
$$

The penalty function has the property that, for most values of $C_1$, $s_1$, $C_2$ and $s_2$, the penalty is large for lengths closer
to the origin, as well as lengths closer to the total length of the data set, but has penalty close to zero for lengths inbetween.
A sample penalty function, with $C_1 = C_2 = 1$, $s_1 = s_2 = 0.3$ and $L = 100$, is plotted below.

```{r}
plot_curve <- function(expr, from, to, points = 100, plot_func=plot, ...) {
  x <- floor(seq(from, to, length.out = 100))
  y <- map_dbl(x, expr)
  plot_func(x, y, ...)
}

plot_curve(~ exp(0.3*(. - 50)) + exp(0.3 * (-. + 50)), from = 0, to = 100, type="l")
```

Given the penalty function general formula, it's necessary to adjust the parameters such the scale of the penalty if
compatible with the order of magnitude of the penalty function. The `auto_penalize` function provided in the Segmentr
package tries to make an educated guess on the order of magnetude of the likelihood, given the dataset it's going to be
applied to, and returns a penalized function. It's possible to to the penalty function using the `small_segment_penalty`
and the `big_segment_penalty`, depending in how much small or big segments, respectively, should be penalized. Therefore,
a `penalized_likelihood` function is created and then used with [segment()].

```{r}
penalized_likelihood <- auto_penalize(berlin, lm_likelihood)
results <- segment(
  berlin,
  likelihood = penalized_likelihood,
  algorithm = "hierarchical"
)
results
```
```{r}
plot_results(results, berlin)
```

The function above found two segments, the last of which was still quite large, which suggests the
`big_segment_penalty` need to be increased to avoid that type of segment in the function.

```{r}
penalized_likelihood <- auto_penalize(berlin, lm_likelihood, big_segment_penalty = 1000)
results <- segment(
  berlin,
  likelihood = penalized_likelihood,
  algorithm = "hierarchical"
)
results
```

```{r}
plot_results(results, berlin)
```

Even though the segments with the new `penalized_likelihood` are of a more moderate size now, it's
still now what was expered compared to what a human observer would identify. The reason behind this
is the nature of the `hierarchical` algorithm used to segment the data. It tries to segment the
data such that, at each changepoint, [segment()] tries to split the data such the combined likelihoods
of the left and right segments are the maximum. However, in order to segment the data ideally,
it's necessary to evaluate all of the possibilities.

The `exact` algorithm does precisely compute all of the possibilities, but its $O(n^2)$ time complexity
is quite prohibitive to run the computation on the entire dataset. An alternative approach, however, would
be to reduze the granularity of the data, getting the monthly averages for each of the the measurement
stations.

## Reducing granularity

The `berlin` dataset needs to be resampled in order to represent the monthly weather averages. It is done by
computing the average temperaturefor each combination of month and weather station. With the granularity reduction,
the dataset will have 24 columns, one for each month in the two years period comprehended in the `berlin`
dataset. The plot of the monthly average temperature of the temperatures of all the data points is plotted below.

```{r}
monthly_berlin <- berlin %>%
  as_tibble(rownames = "station") %>%
  gather(time, temperature, -station) %>%
  mutate(month = floor_date(ymd(time), "month")) %>%
  group_by(station, month) %>%
  summarize(temperature = mean(temperature)) %>%
  spread(month, temperature) %>% {
    stations <- .$station
    result <- as.matrix(.[, -1])
    rownames(result) <- stations
    result
  }

monthly_berlin %>%
  colMeans() %>%
  enframe("time", "temperature") %>%
  mutate_at(vars(time), ymd) %>%
  with(plot(time, temperature))
```

```{r}
penalized_likelihood <- auto_penalize(monthly_berlin, lm_likelihood)

results <- segment(
  monthly_berlin,
  likelihood = penalized_likelihood,
  algorithm = "exact"
)

results
```

```{r}
plot_results(results, monthly_berlin)
```

With the complexity reducion, it's possible to see the data is segmented in a more reasonable manneer,
compared to how a human observer would segment the data.

## A non-usual likelihood function

Take notice the picking of the likelihood just have be able to rank segments in a disired manner.
Therefore, there's freedom to pick even less conventional likelihood function. For example,
the R-squared statistic of the linear regression in each segment is conventionally used to infer
how well the linear model fits the points in the data. It ranges from zero to one and the closer it is
to one, the better it predicts the points in the data. Therefore, a `rsquared_likelihood` is defined
and it can be used to figure what segments better fit linear regressions.

```{r}
rsquared_likelihood <- function (data) {
  as_tibble(t(data)) %>%
    rowid_to_column() %>%
    gather(station, temperature, -rowid) %>%
    with(lm(temperature ~ rowid)) %>%
    summary %>%
    .$adj.r.squared
}

c(rsquared_likelihood(berlin[, 2:3]), rsquared_likelihood(berlin[, 1:150]), rsquared_likelihood(berlin))
```

Similar to the previous case, the new `rsquared_likelihood` has the highest values for small segments. Therefore,
it needs to be penalized with the `auto_penalize` function.

```{r}
penalized_likelihood <- auto_penalize(berlin, rsquared_likelihood)
results <- segment(
  berlin,
  likelihood = penalized_likelihood,
  algorithm = "hierarchical"
)
results
```
```{r}
plot_results(results, berlin)
```

The default penalized `rsquared_likelihood` split the dataset in two segments, approximately of thee same
size. Since the penalty model applied by `auto_penalize` apply the least penalty for values segments
closer to about half the total length, increasing the `big_penalty_segment` will have no effect.
In fact, smaller segments are being over penalized. Because of this, it's necessary to reduce the
`small_segment_penalty` segment.

```{r}
penalized_likelihood <- auto_penalize(berlin, rsquared_likelihood, small_segment_penalty = 1.1)
results <- segment(
  berlin,
  likelihood = penalized_likelihood,
  algorithm = "hierarchical"
)
results
```
```{r}
plot_results(results, berlin)
```

With the adjusted parameters, the new penalized `rsquared_likelihood` was able to segment the data
in a more accurate manner, despite the nature of the hierarchical algorithm. Notice that,
even though it was a good result for the entire dataset, a similarly good result culd possibly not
be obtainer if the result was different. Take for example the segmentation of the data when the
berlin dataset comprehends one year and a half.

```{r}
sub_berlin <- berlin[, 1:547]
penalized_likelihood <- auto_penalize(sub_berlin, rsquared_likelihood, small_segment_penalty = 1.1)
results <- segment(
  sub_berlin,
  likelihood = penalized_likelihood,
  algorithm = "hierarchical"
)
results
```
```{r}
plot_results(results, sub_berlin)
```

The hierarchical algorithm tries approach the problem in a hierarchical manner, first splitting the dataset
in half, then going on to find the points that maximize the linear regressions.
